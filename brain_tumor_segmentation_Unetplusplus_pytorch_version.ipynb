{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brain tumor segmentation -Unetplusplus-pytorch version.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Su17aBtDqiV4",
        "liW2enK1vInH"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNx2HInAthpndLrhpmsm831",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuongkhangduongle/brainMRI_segmentation/blob/main/brain_tumor_segmentation_Unetplusplus_pytorch_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1RvDVUepwjO",
        "outputId": "7c3b7fd1-453b-4604-9b99-8f4c9e9958ac"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun 28 13:14:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIP30X2GqB1h",
        "outputId": "c2ac9050-ee1c-4533-da34-92577b9f69e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSVTDdqWqNgN"
      },
      "source": [
        "!cp '/content/drive/MyDrive/REPORT/CourseProject/CS338/archive.zip' '/content/archive.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A0XR5iSqOYm"
      },
      "source": [
        "!unzip -q './archive.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "easjCNDqqVFw"
      },
      "source": [
        "# Lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ6nO7fwqBEP"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import utils as vutils\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "from torch.optim import SGD, Adam\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from torch.optim import lr_scheduler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su17aBtDqiV4"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sog34xLOqhqM"
      },
      "source": [
        "root = './lgg-mri-segmentation/kaggle_3m'\n",
        "\n",
        "no_mask = 0\n",
        "no_mask_files = []\n",
        "no_file = 0\n",
        "no_files = []\n",
        "num_empty_masks = 0\n",
        "num_nonempty_masks = 0\n",
        "\n",
        "S = 3 # number of pos/neg samples to take\n",
        "empty_mask_samples = []\n",
        "nonempty_mask_samples = []\n",
        "\n",
        "img_dimensions = []\n",
        "msk_dimensions = []\n",
        "\n",
        "n_files = 0\n",
        "for directory in [os.path.join(root,x) for x in os.listdir(root) if os.path.isdir(os.path.join(root, x))]:\n",
        "    for file in os.listdir(directory):\n",
        "        n_files += 1\n",
        "        img_dimensions.append(np.array(cv2.imread(os.path.join(directory, file))).shape)\n",
        "        #count files with no mask\n",
        "        if 'mask' not in file:\n",
        "            #check if mask exists\n",
        "            mask_path = os.path.join(directory, file[:file.find('.tif')]+'_mask.tif')\n",
        "            if not os.path.exists(mask_path):\n",
        "                no_mask += 1\n",
        "                no_mask_files.append(os.path.join(directory, file))\n",
        "        else:\n",
        "            msk_dimensions.append(np.array(cv2.imread(os.path.join(directory, file), cv2.IMREAD_UNCHANGED)).shape)\n",
        "            #count masks with no file\n",
        "            f_path = os.path.join(directory, file[:file.find('mask')-1]+'.tif')\n",
        "            #check if file exists\n",
        "            if not os.path.exists(f_path):\n",
        "                no_file += 1\n",
        "                no_files.append(os.path.join(directory, file))\n",
        "                \n",
        "            #check if mask is empty\n",
        "            j = np.max(cv2.imread(os.path.join(directory, file), cv2.IMREAD_UNCHANGED))\n",
        "            if j > 0:\n",
        "                num_nonempty_masks += 1\n",
        "                if len(nonempty_mask_samples) < S:\n",
        "                    nonempty_mask_samples.append(os.path.join(directory, file))\n",
        "            else:\n",
        "                num_empty_masks += 1\n",
        "                if len(empty_mask_samples) < S:\n",
        "                    empty_mask_samples.append(os.path.join(directory, file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3GWMNGTqon3",
        "outputId": "cc3e8909-786b-4a0a-ece9-760ab67f2b12"
      },
      "source": [
        "#check variance in dimensions\n",
        "print('Image Dimensions')\n",
        "print(len(set(img_dimensions)))\n",
        "print(set(img_dimensions))\n",
        "print('Mask Dimensions')\n",
        "print(len(set(msk_dimensions)))\n",
        "print(set(msk_dimensions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image Dimensions\n",
            "1\n",
            "{(256, 256, 3)}\n",
            "Mask Dimensions\n",
            "1\n",
            "{(256, 256)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1suhSyrJqrAV",
        "outputId": "6a5854b1-cedb-4eb6-821a-79a649edf9ab"
      },
      "source": [
        "print(f'#No Mask: {no_mask}')\n",
        "print(f'#Mask w/ No File: {no_file}')\n",
        "print(f'#Total Files {n_files}')\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Num Samples with Pos/Neg Diagnosis')\n",
        "plt.bar([0,1], [num_empty_masks, num_nonempty_masks], tick_label=['Negative', 'Positive'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#No Mask: 0\n",
            "#Mask w/ No File: 0\n",
            "#Total Files 7858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 2 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu4F860gqznW"
      },
      "source": [
        "#store list of train files for dataset creation of the form (train_img, mask_img, pos_neg_result(1/0))\n",
        "#these tuples are fed into the datasets we create below\n",
        "file_list = []\n",
        "for directory in [os.path.join(root,x) for x in os.listdir(root) if os.path.isdir(os.path.join(root, x))]:\n",
        "    for file in os.listdir(directory):\n",
        "        #add files to list\n",
        "        if 'mask' not in file:\n",
        "            result = 0\n",
        "            img_path = os.path.join(directory, file)\n",
        "            mask_path = os.path.join(directory, file[:file.find('.tif')]+'_mask.tif')\n",
        "                \n",
        "            #check if mask is nonempty\n",
        "            if np.max(cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)) > 0:\n",
        "                result = 1\n",
        "            \n",
        "            file_list.append([img_path, mask_path, result])\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liW2enK1vInH"
      },
      "source": [
        "# EDA V2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayvjlgOovLiC"
      },
      "source": [
        "brain_df = pd.read_csv('/content/lgg-mri-segmentation/kaggle_3m/data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o27AlZhRvOdL"
      },
      "source": [
        "dirr = \"/content/lgg-mri-segmentation/kaggle_3m/\"\n",
        "\n",
        "image_size = 256\n",
        "\n",
        "data = []\n",
        "\n",
        "for sub_dir_path in glob.glob(dirr + \"*\"):\n",
        "    if os.path.isdir(sub_dir_path):\n",
        "        dirname = sub_dir_path.split(\"/\")[-1]\n",
        "        for filename in os.listdir(sub_dir_path):\n",
        "            image_path = sub_dir_path + \"/\" + filename\n",
        "            data.extend([dirname, image_path])\n",
        "        \n",
        "        \n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        \"dirname\" : data[::2],\n",
        "        \"path\" : data[1::2]\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGG13V7BvPu6"
      },
      "source": [
        "df_no_masks = df[~df['path'].str.contains(\"mask\")]\n",
        "df_masks = df[df['path'].str.contains(\"mask\")]\n",
        "\n",
        "no_masks = sorted(df_no_masks[\"path\"].values, key=lambda string: int(string.split('.')[0].split('_')[-1]))\n",
        "masks = sorted(df_masks[\"path\"].values, key=lambda string: int(string.split('.')[0].split('_')[-2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RsOSEewvRLA"
      },
      "source": [
        "df = pd.DataFrame(\n",
        "    {\n",
        "        'patient': df_no_masks['dirname'].values,\n",
        "        'image_path': no_masks,\n",
        "        'mask_path': masks\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18VWJZuCvStk"
      },
      "source": [
        "'''\n",
        "mask > 0: ảnh segment có chứa khối u và ngược lại.\n",
        "'''\n",
        "\n",
        "diagnos = lambda path: 1 if np.max(cv2.imread(path)) > 0 else 0\n",
        "\n",
        "df[\"diagnosis\"] = df[\"mask_path\"].apply(diagnos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxinJzKnvUwm",
        "outputId": "b1ffc4d0-0069-4923-c944-a2deba076bd0"
      },
      "source": [
        "df['diagnosis'].value_counts() / df.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.650547\n",
              "1    0.349453\n",
              "Name: diagnosis, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xA8V3Kxvuz-"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBA3Qg58vXNd",
        "outputId": "62ac7f9d-d913-4f33-9309-0d91c87b2f75"
      },
      "source": [
        "X_train, X_test = train_test_split(df, stratify=df['diagnosis'], test_size=0.15, random_state=42)\n",
        "X_train, X_valid = train_test_split(X_train, stratify=X_train['diagnosis'], test_size=0.5, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "X_valid = X_valid.reset_index(drop=True)\n",
        "\n",
        "print(X_train['diagnosis'].value_counts() / X_train.shape[0])\n",
        "print(X_valid['diagnosis'].value_counts() / X_valid.shape[0])\n",
        "print(X_test['diagnosis'].value_counts() / X_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    0.650689\n",
            "1    0.349311\n",
            "Name: diagnosis, dtype: float64\n",
            "0    0.650299\n",
            "1    0.349701\n",
            "Name: diagnosis, dtype: float64\n",
            "0    0.650847\n",
            "1    0.349153\n",
            "Name: diagnosis, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SuDPQ44q9yq"
      },
      "source": [
        "# Creat Segmentation Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1U4xKWqv3rK"
      },
      "source": [
        "from torch.utils.data import Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCVMjhquq9SE"
      },
      "source": [
        "class Brain_MRI_Segmentation_Dataset(Dataset):\n",
        "    def __init__(self, inputs, transform=None):\n",
        "        self.inputs = inputs\n",
        "        self.transform = transform\n",
        "        self.input_dtype = torch.float32\n",
        "        self.target_dtype = torch.float32\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        #for classification return only the image and the binary label\n",
        "        img_path = self.inputs[index][0]\n",
        "        mask_path = self.inputs[index][1]\n",
        "        #mask_img = cv2.normalize(cv2.imread(mask_path), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "        mask_img = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
        "        x = torch.from_numpy(np.transpose(np.array(cv2.imread(img_path)), (2,0,1))).type(self.input_dtype)\n",
        "        y = torch.from_numpy(np.resize(np.array(mask_img)/255., (1,256,256))).type(self.target_dtype)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            x = self.transform(x)\n",
        "            y = self.transform(y)\n",
        "        \n",
        "        return x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8DFhzMxrIXx"
      },
      "source": [
        "# Custom Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw4jVHVzrHjy"
      },
      "source": [
        "def iou_metric(inputs, targets):\n",
        "    inputs = F.sigmoid(inputs)       \n",
        "        \n",
        "    #flatten label and prediction tensors\n",
        "    inputs = inputs.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "    smooth = 1\n",
        "    #intersection is equivalent to True Positive count\n",
        "    #union is the mutually inclusive area of all labels & predictions \n",
        "    intersection = (inputs * targets).sum()\n",
        "    total = (inputs + targets).sum()\n",
        "    union = total - intersection \n",
        "    \n",
        "    IoU = (intersection + smooth)/(union + smooth)\n",
        "\n",
        "    return IoU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmrWqKpcrMZv"
      },
      "source": [
        "def dice_loss(self, inputs, targets, smooth=1):\n",
        "    #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "    inputs = F.sigmoid(inputs)       \n",
        "\n",
        "    #flatten label and prediction tensors\n",
        "    inputs = inputs.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    intersection = (inputs * targets).sum()                            \n",
        "    dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
        "\n",
        "    return 1 - dice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k50Qj_UBrPuI"
      },
      "source": [
        "def bce_dice_loss(output, target):\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "    return bce(output, target) + dice_loss(output, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsjfPZNRhID4"
      },
      "source": [
        "ALPHA = 0.7\n",
        "BETA = 0.3\n",
        "def tversky_loss(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n",
        "    \n",
        "    #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "    inputs = F.sigmoid(inputs)       \n",
        "    \n",
        "    #flatten label and prediction tensors\n",
        "    inputs = inputs.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "    \n",
        "    #True Positives, False Positives & False Negatives\n",
        "    TP = (inputs * targets).sum()    \n",
        "    FP = ((1-targets) * inputs).sum()\n",
        "    FN = (targets * (1-inputs)).sum()\n",
        "    \n",
        "    Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
        "    \n",
        "    return 1 - Tversky"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k6Az84qwD33"
      },
      "source": [
        "def focal_tversky_loss(inputs, targets, smooth=1, alpha=0.7, beta=0.3, gamma=0.75):\n",
        "    \n",
        "    #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "    inputs = F.sigmoid(inputs)       \n",
        "    \n",
        "    #flatten label and prediction tensors\n",
        "    inputs = inputs.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "    \n",
        "    #True Positives, False Positives & False Negatives\n",
        "    TP = (inputs * targets).sum()    \n",
        "    FP = ((1-targets) * inputs).sum()\n",
        "    FN = (targets * (1-inputs)).sum()\n",
        "    \n",
        "    Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
        "    FocalTversky = (1 - Tversky)**gamma\n",
        "                    \n",
        "    return FocalTversky"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r115YpLvTBY"
      },
      "source": [
        "# Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cNBm6uNvULy"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 0.8))\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVM87R_bvwj2"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsOoyk5NwZPm"
      },
      "source": [
        "positive_diagnoses = [x for x in file_list if x[2] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_OzOHn2wjbh"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32XtcFvgwot8"
      },
      "source": [
        "train, val = train_test_split(positive_diagnoses, test_size = 0.15, random_state = 42)\n",
        "test, val = train_test_split(val, test_size = 0.5, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MmhhOfNvxgG"
      },
      "source": [
        "train_set = Brain_MRI_Segmentation_Dataset(train)\n",
        "val_set = Brain_MRI_Segmentation_Dataset(val)\n",
        "test_set = Brain_MRI_Segmentation_Dataset(test) \n",
        "\n",
        "\n",
        "train_loader = data.DataLoader(dataset=train_set, batch_size=8, shuffle=True)\n",
        "val_loader = data.DataLoader(dataset=val_set, batch_size=8, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_set, batch_size=8, shuffle=False)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPpGkVk2MG7f",
        "outputId": "7a8ba085-5186-4652-d2fd-2813f86fe993"
      },
      "source": [
        "len(train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "146"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VtA-6MXrUDk"
      },
      "source": [
        "# Unet++ Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl5uTLQRr5YA"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class DownConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DownConv, self).__init__()\n",
        "        self.sequence = nn.Sequential(\n",
        "            ConvBlock(in_channels, out_channels),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.sequence(x)\n",
        "        \n",
        "class UpConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UpConv, self).__init__()\n",
        "        self.sequence = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2),\n",
        "            ConvBlock(in_channels, out_channels)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.sequence(x)\n",
        "        \n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "        #input_dim = 256\n",
        "        self.encoder = nn.ModuleList([\n",
        "            DownConv(in_channels, 64), #128\n",
        "            DownConv(64, 128), #64\n",
        "            DownConv(128, 256), #32\n",
        "            DownConv(256, 512) #16\n",
        "        ])\n",
        "        \n",
        "        self.bottleneck = ConvBlock(512, 1024)\n",
        "        \n",
        "        #extra channels allow for concatenation of skip connections in upsampling block\n",
        "        self.decoder = nn.ModuleList([\n",
        "            UpConv(512+1024,512), #32\n",
        "            UpConv(256+512,256), #64\n",
        "            UpConv(128+256,128), #128\n",
        "            UpConv(64+128,64) #256\n",
        "        ])\n",
        "        \n",
        "        self.output_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        o = x\n",
        "        for layer in self.encoder:\n",
        "            o = layer(o)\n",
        "            skips.append(o)\n",
        "        \n",
        "        o = self.bottleneck(o)\n",
        "        \n",
        "        for i, layer in enumerate(self.decoder):\n",
        "            #print(o.size())\n",
        "            o = torch.cat((skips[len(skips)-i-1],o), dim=1)\n",
        "            #print(o.size())\n",
        "            o = layer(o)\n",
        "        \n",
        "        return self.output_conv(o)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmtjYZaDrS-M"
      },
      "source": [
        "class UNetPlusPlus(nn.Module):\n",
        "    def __init__(self, input_channels=3, output_channels=1, depth=5):\n",
        "        super(UNetPlusPlus, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.output_channels = output_channels\n",
        "        self.conv_map = nn.ModuleDict({})\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        \n",
        "        #construct all convolution blocks\n",
        "        #sums in input channels account for concatenated skip connections from previous j nodes\n",
        "        for i in range(self.depth):\n",
        "            for j in range(self.depth-i):\n",
        "                if i == 0 and j == 0:\n",
        "                    self.conv_map[str((i,j))] = ConvBlock(in_channels=input_channels, out_channels=32)\n",
        "                #elif i == 0:\n",
        "                 #   self.conv_map[(i,j)] = ConvBlock(in_channels=(32*j)+64, out_channels=32)\n",
        "                elif j == 0:\n",
        "                    self.conv_map[str((i,j))] = ConvBlock(in_channels=32*(2**(i-1)), out_channels=32*(2**i))\n",
        "                else:\n",
        "                    self.conv_map[str((i,j))] = ConvBlock(in_channels=((32*(2**i))*j)+32*(2**(i+1)), out_channels=32*(2**i))\n",
        "                    \n",
        "        #implement 1x1 convolutions which enable us to compute completed segmentation maps at the supervised nodes:\n",
        "        #X_0,j; j ={1,2,3,...,depth-1}\n",
        "            \n",
        "        self.output_convs = nn.ModuleList()\n",
        "        for j in range(depth-1):\n",
        "            self.output_convs.append(nn.Conv2d(32, output_channels, kernel_size=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = {}\n",
        "        for j in range(self.depth):\n",
        "            for i in range(self.depth-j):\n",
        "                #X_0,0 takes the input image\n",
        "                if i == 0 and j == 0:\n",
        "                    outputs[(i,j)] = self.conv_map[str((i,j))](x)\n",
        "                #X_i,0 convolves over the downsampled output of the previous convolution in the backbone\n",
        "                elif j == 0:\n",
        "                    outputs[(i,j)] = self.conv_map[str((i,j))]( self.pool(outputs[(i-1,j)]) )\n",
        "                else:\n",
        "                    #concatenate all skip connections from same row\n",
        "                    concats = outputs[(i,0)]\n",
        "                    for j_ in range(1,j):\n",
        "                        concats = torch.cat((outputs[(i,j_)], concats), dim=1)\n",
        "                        \n",
        "                    #concatenate upsampled output from lower depth to skip connections\n",
        "                    concats = torch.cat( (concats, self.upsample(outputs[(i+1,j-1)])), dim=1 )\n",
        "                    \n",
        "                    outputs[(i,j)] = self.conv_map[str((i,j))](concats)\n",
        "        \n",
        "        #compute averaged output\n",
        "        dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        averaged_output = torch.zeros(x.size()[0], self.output_channels, x.size()[2], x.size()[3], requires_grad=True, device=dev)\n",
        "        for j in range(1,self.depth):\n",
        "            averaged_output =  averaged_output + self.output_convs[j-1](outputs[(0,j)])\n",
        "        \n",
        "        averaged_output = averaged_output / (self.depth-1)\n",
        "        \n",
        "        return averaged_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3CCQUwsWU1R"
      },
      "source": [
        "class conv_block_nested(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(conv_block_nested, self).__init__()\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        output = self.activation(x)\n",
        "\n",
        "        return output\n",
        "    \n",
        "#Nested Unet\n",
        "\n",
        "class NestedUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of this paper:\n",
        "    https://arxiv.org/pdf/1807.10165.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super(NestedUNet, self).__init__()\n",
        "\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
        "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
        "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
        "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
        "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
        "\n",
        "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0])\n",
        "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
        "        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\n",
        "\n",
        "        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2], filters[1], filters[1])\n",
        "        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])\n",
        "\n",
        "        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1])\n",
        "\n",
        "        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])\n",
        "\n",
        "        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x0_0 = self.conv0_0(x)\n",
        "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(x1_0)], 1))\n",
        "\n",
        "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(x1_1)], 1))\n",
        "\n",
        "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(x1_2)], 1))\n",
        "\n",
        "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(x1_3)], 1))\n",
        "\n",
        "        output = self.final(x0_4)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfJ8crc3xGcH"
      },
      "source": [
        "# Full Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb793P6-rZdg"
      },
      "source": [
        "#train model\n",
        "#batch -\n",
        "#N,3,256,256\n",
        "#N,1,256,256\n",
        "\n",
        "def run_experiment(model_name, model, optimizer, criterion, train_loader, val_loader, device='cuda', num_epochs=50, clear_mem=True):\n",
        "    \n",
        "    #######################\n",
        "    #Train model          #\n",
        "    #######################\n",
        "    print('Model sent to ' + str(device))\n",
        "    model.to(device)\n",
        "    losses = []\n",
        "    train_scores = [] # hold IoU scores\n",
        "    iters = 0\n",
        "    min_loss = 0\n",
        "    PATH = './weight_nested_unet.pt'\n",
        "    for epoch in range(num_epochs):\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        \n",
        "        for i,batch in enumerate(train_loader):\n",
        "            img = batch[0].to(device)\n",
        "            msk = batch[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(img)\n",
        "            loss = criterion(output, msk)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            train_scores.append(iou_metric(output.detach(), msk))\n",
        "\n",
        "            iters += 1\n",
        "\n",
        "            #if iters % 500 == 0:\n",
        "                #print(f'Loss: [{loss}]')\n",
        "                \n",
        "    #for i in range(len(train_scores)):\n",
        "     #   train_scores[i] = train_scores[i].mean()\n",
        "        \n",
        "    #######################\n",
        "    #Validate model       #\n",
        "    #######################\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        val_scores = []\n",
        "\n",
        "        for i,batch in enumerate(val_loader):\n",
        "                img = batch[0].to(device)\n",
        "                msk = batch[1].to(device)\n",
        "\n",
        "                output = model(img)\n",
        "                loss = criterion(output, msk)\n",
        "                val_scores.append(iou_pytorch(output.detach(), msk))\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        if min_loss > loss.item():\n",
        "            min_loss = loss.item()        \n",
        "\n",
        "            torch.save(model.state_dict(), PATH)\n",
        "\n",
        "        results = {\n",
        "            'model_name': model_name,\n",
        "            'train_losses': losses,\n",
        "            'train_scores': train_scores,\n",
        "            'val_losses': val_losses,\n",
        "            'val_scores': val_scores\n",
        "        }\n",
        "\n",
        "        if clear_mem:\n",
        "            del model\n",
        "            del optimizer\n",
        "            del criterion\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETJ3DAemrb78"
      },
      "source": [
        "def plot_training_scores(losses, train_scores):\n",
        "    #plot loss and IoU\n",
        "    fig, axs = plt.subplots(1,2, figsize=(5,5))\n",
        "    axs[0].set_title('Train BCE Loss')\n",
        "    axs[0].plot(range(len(losses)), losses)\n",
        "\n",
        "    axs[1].set_title('IoU Score vs Training Step')\n",
        "    axs[1].plot(range(len(train_scores)), train_scores)\n",
        "\n",
        "def plot_validation_scores(val_losses, val_scores):\n",
        "    #plot loss and IoU scores - we use a histogram because we care about the distribution of losses, not its progression\n",
        "    fig, axs = plt.subplots(1,2, figsize=(7,7))\n",
        "    axs[0].set_title('BCE Loss on Validation Set')\n",
        "    axs[0].hist(val_losses)\n",
        "\n",
        "    temp = [t.cpu().item() for t in val_scores]\n",
        "    axs[1].set_title('IoU Scores on Validation Set')\n",
        "    axs[1].hist(temp)\n",
        "    axs[1].axvline(np.median(np.array(temp)), color='k', linestyle='dashed', linewidth=1)\n",
        "\n",
        "def visualize_segmentation(model, data_loader, num_samples=5, device='cuda'):\n",
        "    #visualize segmentation on unseen samples\n",
        "    fig, axs = plt.subplots(num_samples, 3, figsize=(60,60))\n",
        "\n",
        "    for ax, col in zip(axs[0], ['MRI', 'Ground Truth', 'Predicted Mask']):\n",
        "        ax.set_title(col)\n",
        "\n",
        "    index = 0\n",
        "    for i,batch in enumerate(data_loader):\n",
        "            img = batch[0].to(device)\n",
        "            msk = batch[1].to(device)\n",
        "\n",
        "            output = model(img)\n",
        "\n",
        "            for j in range(batch[0].size()[0]): #iterate over batchsize\n",
        "                axs[index,0].imshow(np.transpose(img[j].detach().cpu().numpy(), (1,2,0)).astype(np.uint8), cmap='bone', interpolation='none')\n",
        "\n",
        "                axs[index,1].imshow(np.transpose(img[j].detach().cpu().numpy(), (1,2,0)).astype(np.uint8), cmap='bone', interpolation='none')\n",
        "                axs[index,1].imshow(torch.squeeze(msk[j]).detach().cpu().numpy(), cmap='Blues', interpolation='none', alpha=0.5)\n",
        "\n",
        "                axs[index,2].imshow(np.transpose(img[j].detach().cpu().numpy(), (1,2,0)).astype(np.uint8), cmap='bone', interpolation='none')\n",
        "                axs[index,2].imshow(torch.squeeze(output[j]).detach().cpu().numpy(), cmap='Greens', interpolation='none', alpha=0.5)\n",
        "\n",
        "                index += 1\n",
        "\n",
        "            if index >= num_samples:\n",
        "                break\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd9_iw2frkh0"
      },
      "source": [
        "# Training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGrqN92JC570"
      },
      "source": [
        "num_epochs = 60\n",
        "model = NestedUnet()\n",
        "optimizer = Adam(model.parameters(), lr=0.05, eps= 0.1)\n",
        "scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.9)\n",
        "criterion = bce_dice_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYKFlhBGrkG4",
        "outputId": "068983dd-0fe1-4f36-c4f4-4458353ff154"
      },
      "source": [
        "results = run_experiment('Unet++_AugmentedData_CE-DiceLoss', model, optimizer, criterion, train_loader, val_loader, device=device, num_epochs=num_epochs, clear_mem=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model sent to cuda\n",
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/60\n",
            "Epoch 21/60\n",
            "Epoch 31/60\n",
            "Epoch 41/60\n",
            "Epoch 51/60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WKOSmKvCeST"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW_68KiGDj5O",
        "outputId": "c04f690f-1d9a-4e0a-f4d0-66efafbf7c07"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "test_scores = []\n",
        "\n",
        "for i,batch in enumerate(test_loader):\n",
        "        img = batch[0].to(device)\n",
        "        msk = batch[1].to(device)\n",
        "\n",
        "        output = model(img)\n",
        "        loss = criterion(output, msk)\n",
        "        test_scores.append(iou_metric(output.detach(), msk))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdoTcWSbD5Qf"
      },
      "source": [
        "test_sc = 0\n",
        "for sc in test_scores:\n",
        "    test_sc += sc.item()\n",
        "\n",
        "test_sc /= len(test_scores)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}